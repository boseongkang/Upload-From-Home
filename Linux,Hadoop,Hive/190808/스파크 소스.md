``` 
[root@master ~]# start-dfs.sh
Starting namenodes on [master]
master: starting namenode, logging to /root/hadoop-2.7.7/logs/hadoop-root-namenode-master.out
slave1: starting datanode, logging to /root/hadoop-2.7.7/logs/hadoop-root-datanode-slave1.out
slave2: starting datanode, logging to /root/hadoop-2.7.7/logs/hadoop-root-datanode-slave2.out
slave3: starting datanode, logging to /root/hadoop-2.7.7/logs/hadoop-root-datanode-slave3.out
Starting secondary namenodes [slave1]
slave1: starting secondarynamenode, logging to /root/hadoop-2.7.7/logs/hadoop-root-secondarynamenode-slave1.out
s[root@master ~]# start-yarn.sh
starting yarn daemons
starting resourcemanager, logging to /root/hadoop-2.7.7/logs/yarn-root-resourcemanager-master.out
slave1: starting nodemanager, logging to /root/hadoop-2.7.7/logs/yarn-root-nodemanager-slave1.out
slave2: starting nodemanager, logging to /root/hadoop-2.7.7/logs/yarn-root-nodemanager-slave2.out
slave3: starting nodemanager, logging to /root/hadoop-2.7.7/logs/yarn-root-nodemanager-slave3.out
[root@master ~]# cd tools
[root@master tools]# wget http://mirror.apache-kr.org/spark/spark-2.4.3-bin-hadoop2.7.tgz
--2019-08-08 10:15:32--  http://mirror.apache-kr.org/spark/spark-2.4.3-bin-hadoop2.7.tgz
Resolving mirror.apache-kr.org (mirror.apache-kr.org)... 1.201.139.179
Connecting to mirror.apache-kr.org (mirror.apache-kr.org)|1.201.139.179|:80... connected.
HTTP request sent, awaiting response... 404 Not Found
2019-08-08 10:15:32 ERROR 404: Not Found.

[root@master tools]# wget http://mirror.apache-kr.org/spark/spark-2.4.3/spark2.4.3-bin-hadoop2.7.tgz
--2019-08-08 10:16:42--  http://mirror.apache-kr.org/spark/spark-2.4.3/spark2.4.3-bin-hadoop2.7.tgz
Resolving mirror.apache-kr.org (mirror.apache-kr.org)... 1.201.139.179
Connecting to mirror.apache-kr.org (mirror.apache-kr.org)|1.201.139.179|:80... connected.
HTTP request sent, awaiting response... 404 Not Found
2019-08-08 10:16:46 ERROR 404: Not Found.

[root@master tools]# wget  http://mirror.apache-kr.org/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz
--2019-08-08 10:17:32--  http://mirror.apache-kr.org/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz
Resolving mirror.apache-kr.org (mirror.apache-kr.org)... 1.201.139.179
Connecting to mirror.apache-kr.org (mirror.apache-kr.org)|1.201.139.179|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 229988313 (219M) [application/x-gzip]
Saving to: ‘spark-2.4.3-bin-hadoop2.7.tgz’

100%[======================================>] 229,988,313 29.1MB/s   in 8.5s   

2019-08-08 10:17:41 (25.7 MB/s) - ‘spark-2.4.3-bin-hadoop2.7.tgz’ saved [229988313/229988313]

[root@master tools]# mv spark-2.4.3-bin-hadoop2.7.tgz spark-2.4.3 ..
mv: cannot stat `spark-2.4.3': 그런 파일이나 디렉터리가 없습니다
[root@master tools]# cd
[root@master ~]# mv spark-2.4.3-bin-hadoop2.7.tgz /tools
[root@master ~]# cd tools
[root@master tools]# ll
합계 958548
-rw-r--r--. 1 root root 232027212  5월 15 04:30 apache-hive-2.3.5-bin.tar.gz
-rwxrw-rw-. 1 root root 345258474  1월 22  2019 eclipse-jee-2018-12-R-linux-gtk-x86_64.tar.gz
-rwxrw-rw-. 1 root root 218720521  7월 31 11:39 hadoop-2.7.7.tar.gz
-rwxrw-rw-. 1 root root 185540433  7월 23 16:04 jdk-8u131-linux-x64.tar.gz
[root@master tools]# tar xvfz apache-hive-2.3.5-bin.tar.gz 
apache-hive-2.3.5-bin/conf/hive-log4j2.properties.template
[root@master tools]# cd
[root@master ~]# ll
합계 40
-rw-------.  1 root   root 1516  7월 31 19:25 anaconda-ks.cfg
drwxr-xr-x. 18 root   root 4096  8월  6 20:55 apache-hive
-rw-r--r--.  1 root   root  621  8월  6 16:53 derby.log
drwxrwxr-x.  8 root   root 4096  8월  5 09:34 eclipse
drwxr-xr-x.  4 root   root   39  8월  1 11:14 eclipse-workspace
drwxr-xr-x. 11 centos ftp  4096  7월 31 17:39 hadoop-2.7.7
-rw-r--r--.  1 root   root 1567  7월 31 10:33 initial-setup-ks.cfg
drwxr-xr-x.  5 root   root 4096  8월  6 16:53 metastore_db
drwxr-xr-x.  2 root   root 4096  8월  6 15:32 sampledata
drwxr-xr-x.  3 root   root 4096  8월  8 10:23 tools
drwxr-xr-x.  2 root   root    6  7월 31 10:35 공개
drwxr-xr-x.  2 root   root    6  7월 31 10:35 다운로드
drwxr-xr-x.  2 root   root    6  7월 31 10:35 문서
drwxr-xr-x.  2 root   root    6  7월 31 10:35 바탕화면
drwxr-xr-x.  2 root   root    6  7월 31 10:35 비디오
drwxr-xr-x.  3 root   root 4096  8월  5 10:37 사진
drwxr-xr-x.  2 root   root    6  7월 31 10:35 서식
drwxr-xr-x.  2 root   root    6  7월 31 10:35 음악
[root@master ~]# cd tools
[root@master tools]# wget  http://mirror.apache-kr.org/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz
--2019-08-08 10:25:21--  http://mirror.apache-kr.org/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz
Resolving mirror.apache-kr.org (mirror.apache-kr.org)... 1.201.139.179
Connecting to mirror.apache-kr.org (mirror.apache-kr.org)|1.201.139.179|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 229988313 (219M) [application/x-gzip]
Saving to: ‘spark-2.4.3-bin-hadoop2.7.tgz’

100%[======================================>] 229,988,313 20.7MB/s   in 9.4s   

2019-08-08 10:25:31 (23.3 MB/s) - ‘spark-2.4.3-bin-hadoop2.7.tgz’ saved [229988313/229988313]

[root@master tools]# tar xvfz spark-2.4.3-bin-hadoop2.7.tgz 
spark-2.4.3-bin-hadoop2.7/
spark-2.4.3-bin-hadoop2.7/python/

spark-2.4.3-bin-hadoop2.7/LICENSE
[root@master tools]# mv spark-2.4.3-bin-hadoop2.7 spark-2.4.3 /root
mv: cannot stat `spark-2.4.3': 그런 파일이나 디렉터리가 없습니다
[root@master tools]# mv spark-2.4.3-bin-hadoop2.7 spark-2.4.3 
mv: cannot stat `spark-2.4.3-bin-hadoop2.7': 그런 파일이나 디렉터리가 없습니다
[root@master tools]# mv spark-2.4.3-bin-hadoop2.7.tgz spark-2.4.3
[root@master tools]# ll
합계 1183152
drwxr-xr-x. 10 root root      4096  8월  8 10:23 apache-hive-2.3.5-bin
-rw-r--r--.  1 root root 232027212  5월 15 04:30 apache-hive-2.3.5-bin.tar.gz
-rwxrw-rw-.  1 root root 345258474  1월 22  2019 eclipse-jee-2018-12-R-linux-gtk-x86_64.tar.gz
-rwxrw-rw-.  1 root root 218720521  7월 31 11:39 hadoop-2.7.7.tar.gz
-rwxrw-rw-.  1 root root 185540433  7월 23 16:04 jdk-8u131-linux-x64.tar.gz
-rw-r--r--.  1 root root 229988313  5월  1 14:57 spark-2.4.3
[root@master tools]# mv spark-2.4.3 /root
[root@master tools]# cd ..
[root@master ~]# ll
합계 302796
-rw-------.  1 root   root        1516  7월 31 19:25 anaconda-ks.cfg
drwxr-xr-x. 18 root   root        4096  8월  6 20:55 apache-hive
-rw-r--r--.  1 root   root         621  8월  6 16:53 derby.log
drwxrwxr-x.  8 root   root        4096  8월  5 09:34 eclipse
drwxr-xr-x.  4 root   root          39  8월  1 11:14 eclipse-workspace
drwxr-xr-x. 11 centos ftp         4096  7월 31 17:39 hadoop-2.7.7
-rw-r--r--.  1 root   root        1567  7월 31 10:33 initial-setup-ks.cfg
drwxr-xr-x.  5 root   root        4096  8월  6 16:53 metastore_db
drwxr-xr-x.  2 root   root        4096  8월  6 15:32 sampledata
-rw-r--r--.  1 root   root   229988313  5월  1 14:57 spark-2.4.3
drwxr-xr-x. 13 centos centos      4096  5월  1 14:19 spark-2.4.3-bin-hadoop2.7
-rw-r--r--.  1 root   root    80026692  8월  8 10:25 spark-2.4.3-bin-hadoop2.7.tgz
drwxr-xr-x.  3 root   root        4096  8월  8 10:30 tools
drwxr-xr-x.  2 root   root           6  7월 31 10:35 공개
drwxr-xr-x.  2 root   root           6  7월 31 10:35 다운로드
drwxr-xr-x.  2 root   root           6  7월 31 10:35 문서
drwxr-xr-x.  2 root   root           6  7월 31 10:35 바탕화면
drwxr-xr-x.  2 root   root           6  7월 31 10:35 비디오
drwxr-xr-x.  3 root   root        4096  8월  5 10:37 사진
drwxr-xr-x.  2 root   root           6  7월 31 10:35 서식
drwxr-xr-x.  2 root   root           6  7월 31 10:35 음악
[root@master ~]# vi .bashrc
[root@master ~]# cd tools
[root@master tools]# vi .bashrc
[root@master tools]# cd ..
[root@master ~]# vi .bashrc

[1]+  Stopped                 vi .bashrc
[root@master ~]# ll
합계 302796
-rw-------.  1 root   root        1516  7월 31 19:25 anaconda-ks.cfg
drwxr-xr-x. 18 root   root        4096  8월  6 20:55 apache-hive
-rw-r--r--.  1 root   root         621  8월  6 16:53 derby.log
drwxrwxr-x.  8 root   root        4096  8월  5 09:34 eclipse
drwxr-xr-x.  4 root   root          39  8월  1 11:14 eclipse-workspace
drwxr-xr-x. 11 centos ftp         4096  7월 31 17:39 hadoop-2.7.7
-rw-r--r--.  1 root   root        1567  7월 31 10:33 initial-setup-ks.cfg
drwxr-xr-x.  5 root   root        4096  8월  6 16:53 metastore_db
drwxr-xr-x.  2 root   root        4096  8월  6 15:32 sampledata
-rw-r--r--.  1 root   root   229988313  5월  1 14:57 spark-2.4.3
drwxr-xr-x. 13 centos centos      4096  5월  1 14:19 spark-2.4.3-bin-hadoop2.7
-rw-r--r--.  1 root   root    80026692  8월  8 10:25 spark-2.4.3-bin-hadoop2.7.tgz
drwxr-xr-x.  3 root   root        4096  8월  8 10:32 tools
drwxr-xr-x.  2 root   root           6  7월 31 10:35 공개
drwxr-xr-x.  2 root   root           6  7월 31 10:35 다운로드
drwxr-xr-x.  2 root   root           6  7월 31 10:35 문서
drwxr-xr-x.  2 root   root           6  7월 31 10:35 바탕화면
drwxr-xr-x.  2 root   root           6  7월 31 10:35 비디오
drwxr-xr-x.  3 root   root        4096  8월  5 10:37 사진
drwxr-xr-x.  2 root   root           6  7월 31 10:35 서식
drwxr-xr-x.  2 root   root           6  7월 31 10:35 음악
[root@master ~]# vi .bashrc
[root@master ~]# vi .bashrc
[root@master ~]# vi .bashrc
[root@master ~]# cd tools
[root@master tools]# vi .bashrc
[root@master tools]# gedit .bashrc
[root@master tools]# cd ..
[root@master ~]# source .bashrc
[root@master ~]# vi .bashrc
[root@master ~]# cd tools
[root@master tools]# vi .bashrc
[root@master tools]# rm -f .bashrc
[root@master tools]# ls
apache-hive-2.3.5-bin                          hadoop-2.7.7.tar.gz
apache-hive-2.3.5-bin.tar.gz                   jdk-8u131-linux-x64.tar.gz
eclipse-jee-2018-12-R-linux-gtk-x86_64.tar.gz
[root@master tools]# cd ..
[root@master ~]# ls
anaconda-ks.cfg    initial-setup-ks.cfg           tools     사진
apache-hive        metastore_db                   공개      서식
derby.log          sampledata                     다운로드  음악
eclipse            spark-2.4.3                    문서
eclipse-workspace  spark-2.4.3-bin-hadoop2.7      바탕화면
hadoop-2.7.7       spark-2.4.3-bin-hadoop2.7.tgz  비디오
[root@master ~]# ll
합계 302796
-rw-------.  1 root   root        1516  7월 31 19:25 anaconda-ks.cfg
drwxr-xr-x. 18 root   root        4096  8월  6 20:55 apache-hive
-rw-r--r--.  1 root   root         621  8월  6 16:53 derby.log
drwxrwxr-x.  8 root   root        4096  8월  5 09:34 eclipse
drwxr-xr-x.  4 root   root          39  8월  1 11:14 eclipse-workspace
drwxr-xr-x. 11 centos ftp         4096  7월 31 17:39 hadoop-2.7.7
-rw-r--r--.  1 root   root        1567  7월 31 10:33 initial-setup-ks.cfg
drwxr-xr-x.  5 root   root        4096  8월  6 16:53 metastore_db
drwxr-xr-x.  2 root   root        4096  8월  6 15:32 sampledata
-rw-r--r--.  1 root   root   229988313  5월  1 14:57 spark-2.4.3
drwxr-xr-x. 13 centos centos      4096  5월  1 14:19 spark-2.4.3-bin-hadoop2.7
-rw-r--r--.  1 root   root    80026692  8월  8 10:25 spark-2.4.3-bin-hadoop2.7.tgz
drwxr-xr-x.  3 root   root        4096  8월  8 10:40 tools
drwxr-xr-x.  2 root   root           6  7월 31 10:35 공개
drwxr-xr-x.  2 root   root           6  7월 31 10:35 다운로드
drwxr-xr-x.  2 root   root           6  7월 31 10:35 문서
drwxr-xr-x.  2 root   root           6  7월 31 10:35 바탕화면
drwxr-xr-x.  2 root   root           6  7월 31 10:35 비디오
drwxr-xr-x.  3 root   root        4096  8월  5 10:37 사진
drwxr-xr-x.  2 root   root           6  7월 31 10:35 서식
drwxr-xr-x.  2 root   root           6  7월 31 10:35 음악
[root@master ~]# vi .bashrc
[root@master ~]# cd tools
[root@master tools]# ll
합계 958552
drwxr-xr-x. 10 root root      4096  8월  8 10:23 apache-hive-2.3.5-bin
-rw-r--r--.  1 root root 232027212  5월 15 04:30 apache-hive-2.3.5-bin.tar.gz
-rwxrw-rw-.  1 root root 345258474  1월 22  2019 eclipse-jee-2018-12-R-linux-gtk-x86_64.tar.gz
-rwxrw-rw-.  1 root root 218720521  7월 31 11:39 hadoop-2.7.7.tar.gz
-rwxrw-rw-.  1 root root 185540433  7월 23 16:04 jdk-8u131-linux-x64.tar.gz
[root@master tools]# vi .bashrc
[root@master tools]# cd ..
[root@master ~]# cat .bashrc
# .bashrc

# User specific aliases and functions

alias rm='rm -i'
alias cp='cp -i'
alias mv='mv -i'

# Source global definitions
if [ -f /etc/bashrc ]; then
	. /etc/bashrc
fi
export HADOOP_HOME=/root/hadoop-2.7.7
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
export SPARK_HOME=/root/spark-2.4.3
export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH
export HIVE_HOME=/root/apache-hive
export PATH=$HIVE_HOME/bin:$PATH

[root@master ~]# vi .bashrc
[root@master ~]# vi .bashrc
[root@master ~]# cd SPARK_HOME /conf
bash: cd: SPARK_HOME: 그런 파일이나 디렉터리가 없습니다
[root@master ~]# cd $SPARK_HOME /conf
bash: cd: /root/spark-2.4.3: 디렉터리가 아닙니다
[root@master ~]# cd $SPARK_HOME/conf
bash: cd: /root/spark-2.4.3/conf: 디렉터리가 아닙니다
[root@master ~]# ls
anaconda-ks.cfg    initial-setup-ks.cfg           tools     사진
apache-hive        metastore_db                   공개      서식
derby.log          sampledata                     다운로드  음악
eclipse            spark-2.4.3                    문서
eclipse-workspace  spark-2.4.3-bin-hadoop2.7      바탕화면
hadoop-2.7.7       spark-2.4.3-bin-hadoop2.7.tgz  비디오
[root@master ~]# cd spark-2.4.3-bin-hadoop2.7
[root@master spark-2.4.3-bin-hadoop2.7]# ll
합계 112
-rw-r--r--. 1 centos centos 21316  5월  1 14:19 LICENSE
-rw-r--r--. 1 centos centos 42919  5월  1 14:19 NOTICE
drwxr-xr-x. 3 centos centos    16  5월  1 14:19 R
-rw-r--r--. 1 centos centos  3952  5월  1 14:19 README.md
-rw-r--r--. 1 centos centos   164  5월  1 14:19 RELEASE
drwxr-xr-x. 2 centos centos  4096  5월  1 14:19 bin
drwxr-xr-x. 2 centos centos  4096  5월  1 14:19 conf
drwxr-xr-x. 5 centos centos    47  5월  1 14:19 data
drwxr-xr-x. 4 centos centos    27  5월  1 14:19 examples
drwxr-xr-x. 2 centos centos 12288  5월  1 14:19 jars
drwxr-xr-x. 4 centos centos    36  5월  1 14:19 kubernetes
drwxr-xr-x. 2 centos centos  4096  5월  1 14:19 licenses
drwxr-xr-x. 9 centos centos  4096  5월  1 14:19 python
drwxr-xr-x. 2 centos centos  4096  5월  1 14:19 sbin
drwxr-xr-x. 2 centos centos    41  5월  1 14:19 yarn
[root@master spark-2.4.3-bin-hadoop2.7]# ls
LICENSE  R          RELEASE  conf  examples  kubernetes  python  yarn
NOTICE   README.md  bin      data  jars      licenses    sbin
[root@master spark-2.4.3-bin-hadoop2.7]# cd conf
[root@master conf]# ll
합계 36
-rw-r--r--. 1 centos centos  996  5월  1 14:19 docker.properties.template
-rw-r--r--. 1 centos centos 1105  5월  1 14:19 fairscheduler.xml.template
-rw-r--r--. 1 centos centos 2025  5월  1 14:19 log4j.properties.template
-rw-r--r--. 1 centos centos 7801  5월  1 14:19 metrics.properties.template
-rw-r--r--. 1 centos centos  865  5월  1 14:19 slaves.template
-rw-r--r--. 1 centos centos 1292  5월  1 14:19 spark-defaults.conf.template
-rwxr-xr-x. 1 centos centos 4221  5월  1 14:19 spark-env.sh.template
[root@master conf]# cd ..
[root@master spark-2.4.3-bin-hadoop2.7]# cd ..
[root@master ~]# cd $SPARK_HOME
bash: cd: /root/spark-2.4.3: 디렉터리가 아닙니다
[root@master ~]# cd$SPARK_HOME
bash: cd/root/spark-2.4.3: 그런 파일이나 디렉터리가 없습니다
[root@master ~]# cd $SPARK_HOME/
bash: cd: /root/spark-2.4.3/: 디렉터리가 아닙니다
[root@master ~]# ll
합계 302796
-rw-------.  1 root   root        1516  7월 31 19:25 anaconda-ks.cfg
drwxr-xr-x. 18 root   root        4096  8월  6 20:55 apache-hive
-rw-r--r--.  1 root   root         621  8월  6 16:53 derby.log
drwxrwxr-x.  8 root   root        4096  8월  5 09:34 eclipse
drwxr-xr-x.  4 root   root          39  8월  1 11:14 eclipse-workspace
drwxr-xr-x. 11 centos ftp         4096  7월 31 17:39 hadoop-2.7.7
-rw-r--r--.  1 root   root        1567  7월 31 10:33 initial-setup-ks.cfg
drwxr-xr-x.  5 root   root        4096  8월  6 16:53 metastore_db
drwxr-xr-x.  2 root   root        4096  8월  6 15:32 sampledata
-rw-r--r--.  1 root   root   229988313  5월  1 14:57 spark-2.4.3
drwxr-xr-x. 13 centos centos      4096  5월  1 14:19 spark-2.4.3-bin-hadoop2.7
-rw-r--r--.  1 root   root    80026692  8월  8 10:25 spark-2.4.3-bin-hadoop2.7.tgz
drwxr-xr-x.  3 root   root        4096  8월  8 10:42 tools
drwxr-xr-x.  2 root   root           6  7월 31 10:35 공개
drwxr-xr-x.  2 root   root           6  7월 31 10:35 다운로드
drwxr-xr-x.  2 root   root           6  7월 31 10:35 문서
drwxr-xr-x.  2 root   root           6  7월 31 10:35 바탕화면
drwxr-xr-x.  2 root   root           6  7월 31 10:35 비디오
drwxr-xr-x.  3 root   root        4096  8월  5 10:37 사진
drwxr-xr-x.  2 root   root           6  7월 31 10:35 서식
drwxr-xr-x.  2 root   root           6  7월 31 10:35 음악
[root@master ~]# ls
anaconda-ks.cfg    initial-setup-ks.cfg           tools     사진
apache-hive        metastore_db                   공개      서식
derby.log          sampledata                     다운로드  음악
eclipse            spark-2.4.3                    문서
eclipse-workspace  spark-2.4.3-bin-hadoop2.7      바탕화면
hadoop-2.7.7       spark-2.4.3-bin-hadoop2.7.tgz  비디오
[root@master ~]# cd $spark_home
[root@master ~]# pwd
/root
[root@master ~]# ls
anaconda-ks.cfg    initial-setup-ks.cfg           tools     사진
apache-hive        metastore_db                   공개      서식
derby.log          sampledata                     다운로드  음악
eclipse            spark-2.4.3                    문서
eclipse-workspace  spark-2.4.3-bin-hadoop2.7      바탕화면
hadoop-2.7.7       spark-2.4.3-bin-hadoop2.7.tgz  비디오
[root@master ~]# cd spark-2.4.3
bash: cd: spark-2.4.3: 디렉터리가 아닙니다
[root@master ~]# cd $SPARK_HOME 
bash: cd: /root/spark-2.4.3: 디렉터리가 아닙니다
[root@master ~]# cd tools
[root@master tools]# ll
합계 958552
drwxr-xr-x. 10 root root      4096  8월  8 10:23 apache-hive-2.3.5-bin
-rw-r--r--.  1 root root 232027212  5월 15 04:30 apache-hive-2.3.5-bin.tar.gz
-rwxrw-rw-.  1 root root 345258474  1월 22  2019 eclipse-jee-2018-12-R-linux-gtk-x86_64.tar.gz
-rwxrw-rw-.  1 root root 218720521  7월 31 11:39 hadoop-2.7.7.tar.gz
-rwxrw-rw-.  1 root root 185540433  7월 23 16:04 jdk-8u131-linux-x64.tar.gz
[root@master tools]# ls
apache-hive-2.3.5-bin                          hadoop-2.7.7.tar.gz
apache-hive-2.3.5-bin.tar.gz                   jdk-8u131-linux-x64.tar.gz
eclipse-jee-2018-12-R-linux-gtk-x86_64.tar.gz
[root@master tools]# cd ..
[root@master ~]# ll
합계 302796
-rw-------.  1 root   root        1516  7월 31 19:25 anaconda-ks.cfg
drwxr-xr-x. 18 root   root        4096  8월  6 20:55 apache-hive
-rw-r--r--.  1 root   root         621  8월  6 16:53 derby.log
drwxrwxr-x.  8 root   root        4096  8월  5 09:34 eclipse
drwxr-xr-x.  4 root   root          39  8월  1 11:14 eclipse-workspace
drwxr-xr-x. 11 centos ftp         4096  7월 31 17:39 hadoop-2.7.7
-rw-r--r--.  1 root   root        1567  7월 31 10:33 initial-setup-ks.cfg
drwxr-xr-x.  5 root   root        4096  8월  6 16:53 metastore_db
drwxr-xr-x.  2 root   root        4096  8월  6 15:32 sampledata
-rw-r--r--.  1 root   root   229988313  5월  1 14:57 spark-2.4.3
drwxr-xr-x. 13 centos centos      4096  5월  1 14:19 spark-2.4.3-bin-hadoop2.7
-rw-r--r--.  1 root   root    80026692  8월  8 10:25 spark-2.4.3-bin-hadoop2.7.tgz
drwxr-xr-x.  3 root   root        4096  8월  8 10:42 tools
drwxr-xr-x.  2 root   root           6  7월 31 10:35 공개
drwxr-xr-x.  2 root   root           6  7월 31 10:35 다운로드
drwxr-xr-x.  2 root   root           6  7월 31 10:35 문서
drwxr-xr-x.  2 root   root           6  7월 31 10:35 바탕화면
drwxr-xr-x.  2 root   root           6  7월 31 10:35 비디오
drwxr-xr-x.  3 root   root        4096  8월  5 10:37 사진
drwxr-xr-x.  2 root   root           6  7월 31 10:35 서식
drwxr-xr-x.  2 root   root           6  7월 31 10:35 음악
[root@master ~]# ls
anaconda-ks.cfg    initial-setup-ks.cfg           tools     사진
apache-hive        metastore_db                   공개      서식
derby.log          sampledata                     다운로드  음악
eclipse            spark-2.4.3                    문서
eclipse-workspace  spark-2.4.3-bin-hadoop2.7      바탕화면
hadoop-2.7.7       spark-2.4.3-bin-hadoop2.7.tgz  비디오
[root@master ~]# cd $SPARK_HOME
bash: cd: /root/spark-2.4.3: 디렉터리가 아닙니다
[root@master ~]# vi .bashrc
[root@master ~]# ls
anaconda-ks.cfg    initial-setup-ks.cfg           tools     사진
apache-hive        metastore_db                   공개      서식
derby.log          sampledata                     다운로드  음악
eclipse            spark-2.4.3                    문서
eclipse-workspace  spark-2.4.3-bin-hadoop2.7      바탕화면
hadoop-2.7.7       spark-2.4.3-bin-hadoop2.7.tgz  비디오
[root@master ~]# stop-yarn.sh
stopping yarn daemons
stopping resourcemanager
slave3: stopping nodemanager
slave2: stopping nodemanager
slave1: stopping nodemanager
no proxyserver to stop
[root@master ~]# start-yarn.sh
starting yarn daemons
starting resourcemanager, logging to /root/hadoop-2.7.7/logs/yarn-root-resourcemanager-master.out
slave2: starting nodemanager, logging to /root/hadoop-2.7.7/logs/yarn-root-nodemanager-slave2.out
slave1: starting nodemanager, logging to /root/hadoop-2.7.7/logs/yarn-root-nodemanager-slave1.out
slave3: starting nodemanager, logging to /root/hadoop-2.7.7/logs/yarn-root-nodemanager-slave3.out
[root@master ~]# cd tools
[root@master tools]# ll
합계 958552
drwxr-xr-x. 10 root root      4096  8월  8 10:23 apache-hive-2.3.5-bin
-rw-r--r--.  1 root root 232027212  5월 15 04:30 apache-hive-2.3.5-bin.tar.gz
-rwxrw-rw-.  1 root root 345258474  1월 22  2019 eclipse-jee-2018-12-R-linux-gtk-x86_64.tar.gz
-rwxrw-rw-.  1 root root 218720521  7월 31 11:39 hadoop-2.7.7.tar.gz
-rwxrw-rw-.  1 root root 185540433  7월 23 16:04 jdk-8u131-linux-x64.tar.gz
[root@master tools]# cd ..
[root@master ~]# cd $SPARK_HOME
bash: cd: /root/spark-2.4.3: 디렉터리가 아닙니다
[root@master ~]# ll
합계 302796
-rw-------.  1 root   root        1516  7월 31 19:25 anaconda-ks.cfg
drwxr-xr-x. 18 root   root        4096  8월  6 20:55 apache-hive
-rw-r--r--.  1 root   root         621  8월  6 16:53 derby.log
drwxrwxr-x.  8 root   root        4096  8월  5 09:34 eclipse
drwxr-xr-x.  4 root   root          39  8월  1 11:14 eclipse-workspace
drwxr-xr-x. 11 centos ftp         4096  7월 31 17:39 hadoop-2.7.7
-rw-r--r--.  1 root   root        1567  7월 31 10:33 initial-setup-ks.cfg
drwxr-xr-x.  5 root   root        4096  8월  6 16:53 metastore_db
drwxr-xr-x.  2 root   root        4096  8월  6 15:32 sampledata
-rw-r--r--.  1 root   root   229988313  5월  1 14:57 spark-2.4.3
drwxr-xr-x. 13 centos centos      4096  5월  1 14:19 spark-2.4.3-bin-hadoop2.7
-rw-r--r--.  1 root   root    80026692  8월  8 10:25 spark-2.4.3-bin-hadoop2.7.tgz
drwxr-xr-x.  3 root   root        4096  8월  8 10:42 tools
drwxr-xr-x.  2 root   root           6  7월 31 10:35 공개
drwxr-xr-x.  2 root   root           6  7월 31 10:35 다운로드
drwxr-xr-x.  2 root   root           6  7월 31 10:35 문서
drwxr-xr-x.  2 root   root           6  7월 31 10:35 바탕화면
drwxr-xr-x.  2 root   root           6  7월 31 10:35 비디오
drwxr-xr-x.  3 root   root        4096  8월  5 10:37 사진
drwxr-xr-x.  2 root   root           6  7월 31 10:35 서식
drwxr-xr-x.  2 root   root           6  7월 31 10:35 음악
[root@master ~]# cd tools
[root@master tools]# ls
apache-hive-2.3.5-bin                          hadoop-2.7.7.tar.gz
apache-hive-2.3.5-bin.tar.gz                   jdk-8u131-linux-x64.tar.gz
eclipse-jee-2018-12-R-linux-gtk-x86_64.tar.gz
[root@master tools]# cd
[root@master ~]# cd spark-2.4.3
bash: cd: spark-2.4.3: 디렉터리가 아닙니다
[root@master ~]# rm -f spark-2.4.3
[root@master ~]# mv spark-2.4.3-bin-hadoop2.7 spark-2.4.3
[root@master ~]# ll
합계 78196
-rw-------.  1 root   root       1516  7월 31 19:25 anaconda-ks.cfg
drwxr-xr-x. 18 root   root       4096  8월  6 20:55 apache-hive
-rw-r--r--.  1 root   root        621  8월  6 16:53 derby.log
drwxrwxr-x.  8 root   root       4096  8월  5 09:34 eclipse
drwxr-xr-x.  4 root   root         39  8월  1 11:14 eclipse-workspace
drwxr-xr-x. 11 centos ftp        4096  7월 31 17:39 hadoop-2.7.7
-rw-r--r--.  1 root   root       1567  7월 31 10:33 initial-setup-ks.cfg
drwxr-xr-x.  5 root   root       4096  8월  6 16:53 metastore_db
drwxr-xr-x.  2 root   root       4096  8월  6 15:32 sampledata
drwxr-xr-x. 13 centos centos     4096  5월  1 14:19 spark-2.4.3
-rw-r--r--.  1 root   root   80026692  8월  8 10:25 spark-2.4.3-bin-hadoop2.7.tgz
drwxr-xr-x.  3 root   root       4096  8월  8 10:42 tools
drwxr-xr-x.  2 root   root          6  7월 31 10:35 공개
drwxr-xr-x.  2 root   root          6  7월 31 10:35 다운로드
drwxr-xr-x.  2 root   root          6  7월 31 10:35 문서
drwxr-xr-x.  2 root   root          6  7월 31 10:35 바탕화면
drwxr-xr-x.  2 root   root          6  7월 31 10:35 비디오
drwxr-xr-x.  3 root   root       4096  8월  5 10:37 사진
drwxr-xr-x.  2 root   root          6  7월 31 10:35 서식
drwxr-xr-x.  2 root   root          6  7월 31 10:35 음악
[root@master ~]# cd $SPARK_HOME
[root@master spark-2.4.3]# cd conf
[root@master conf]# pv spark-env.sh.template spark-env.sh
bash: pv: 명령을 찾을 수 없습니다...
[root@master conf]# cp spark-env.sh.template spark-env.sh
[root@master conf]# vi spark-env.sh
[root@master conf]# cp log4j.properties.template log4j.properties
[root@master conf]# vi log4j.properties
[root@master conf]# cd 
[root@master ~]# spark-shell
19/08/08 11:13:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context Web UI available at http://master:4040
Spark context available as 'sc' (master = local[*], app id = local-1565230413918).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.3
      /_/
         
Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_131)
Type in expressions to have them evaluated.
Type :help for more information.

scala> sc
res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@44a1ae4e

scala> val textFile = sc.textFile("hdfs://192
<console>:1: error: unclosed string literal
val textFile = sc.textFile("hdfs://192
                           ^

scala> val textFile = sc.textFile("hdfs://192.168.111.120:9000/edudata/president_moon.txt")
textFile: org.apache.spark.rdd.RDD[String] = hdfs://192.168.111.120:9000/edudata/president_moon.txt MapPartitionsRDD[1] at textFile at <console>:24

scala> val counts = textFile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_ + _)
counts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at <console>:25

scala> counts.collect()
res1: 내용은 지움
scala> counts.saveAsTextFile("/output/scalaresult")
                                                                                
scala> val rdd1 = sc.textFile("/edudata/fruit.txt")
rdd1: org.apache.spark.rdd.RDD[String] = /edudata/fruit.txt MapPartitionsRDD[7] at textFile at <console>:24

scala> rdd1.collect()
res3: Array[String] = Array(apple banana cherry peach, banana cherry peach  cherry  cherry, apple cherry peach grape, apple grape pear cherry peach, orange banana cherry orange, apple banana pear banana peach banana, orange banana orange peach orange orange, apple banana pear  pear  pear peach, apple tomato cherry peach, tomato tomato tomato tomato)

scala> counts.saveAsTextFile("/edudata/fruit.txt")
org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://master:9000/edudata/fruit.txt already exists
  at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)
  at org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:287)
  at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)
  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)
  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)
  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)
  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)
  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)
  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)
  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)
  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)
  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)
  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1499)
  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)
  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)
  ... 49 elided

scala> val rdd2 = rdd1.flatMap(line => line.split(" "))
rdd2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[9] at flatMap at <console>:25

scala> rdd2.collect()
res5: Array[String] = Array(apple, banana, cherry, peach, banana, cherry, peach, "", cherry, "", cherry, apple, cherry, peach, grape, apple, grape, pear, cherry, peach, orange, banana, cherry, orange, apple, banana, pear, banana, peach, banana, orange, banana, orange, peach, orange, orange, apple, banana, pear, "", pear, "", pear, peach, apple, tomato, cherry, peach, tomato, tomato, tomato, tomato)

scala> val rdd3 = rdd2.map(word => (word, 1))
rdd3: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[10] at map at <console>:25

scala> rdd3.collect()
res6: Array[(String, Int)] = Array((apple,1), (banana,1), (cherry,1), (peach,1), (banana,1), (cherry,1), (peach,1), ("",1), (cherry,1), ("",1), (cherry,1), (apple,1), (cherry,1), (peach,1), (grape,1), (apple,1), (grape,1), (pear,1), (cherry,1), (peach,1), (orange,1), (banana,1), (cherry,1), (orange,1), (apple,1), (banana,1), (pear,1), (banana,1), (peach,1), (banana,1), (orange,1), (banana,1), (orange,1), (peach,1), (orange,1), (orange,1), (apple,1), (banana,1), (pear,1), ("",1), (pear,1), ("",1), (pear,1), (peach,1), (apple,1), (tomato,1), (cherry,1), (peach,1), (tomato,1), (tomato,1), (tomato,1), (tomato,1))

scala> val rdd4 = rdd3.reduceByKey(_ + _)
rdd4: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[11] at reduceByKey at <console>:25

scala> rdd4.collect()
res7: Array[(String, Int)] = Array((banana,8), (pear,5), (orange,6), (grape,2), (cherry,8), (tomato,5), ("",4), (peach,8), (apple,6))

scala> [root@master ~]# pwd
/root
[root@master ~]# cd $SPARK_HOME
[root@master spark-2.4.3]# ls
LICENSE  R          RELEASE  conf  examples  kubernetes  python  yarn
NOTICE   README.md  bin      data  jars      licenses    sbin
[root@master spark-2.4.3]# mkdir myspark 
[root@master spark-2.4.3]# ls
LICENSE  R          RELEASE  conf  examples  kubernetes  myspark  sbin
NOTICE   README.md  bin      data  jars      licenses    python   yarn
[root@master spark-2.4.3]# cd myspark
[root@master myspark]# pwd
/root/spark-2.4.3/myspark
[root@master myspark]# ls
wc7.jar
[root@master myspark]# jar tvf wc7.jar
    25 Thu Aug 08 14:43:58 KST 2019 META-INF/MANIFEST.MF
  1219 Thu Aug 08 14:13:14 KST 2019 sparkexam/WordCount7$1.class
  1143 Thu Aug 08 14:13:14 KST 2019 sparkexam/WordCount7$2.class
  1044 Thu Aug 08 14:13:14 KST 2019 sparkexam/WordCount7$3.class
  2245 Thu Aug 08 14:13:14 KST 2019 sparkexam/WordCount7.class
[root@master myspark]# spark-submit --class sparkexam.WordCount7 wc7.jar
19/08/08 14:46:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/08/08 14:46:22 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
19/08/08 14:47:16 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[(pear,5), (orange,6), (tomato,5), (,4), (apple,6), (banana,8), (grape,2), (cherry,8), (peach,8)]
[root@master myspark]# hdfs dfs -ls
Found 1 items
drwxr-xr-x   - root supergroup          0 2019-08-08 14:47 .sparkStaging
[root@master myspark]# hdfs dfs -ll
-ll: Unknown command
[root@master myspark]# hdfs dfs -ls
Found 1 items
drwxr-xr-x   - root supergroup          0 2019-08-08 14:47 .sparkStaging
[root@master myspark]# ls
wc7.jar
[root@master myspark]# ls
wc7.jar
[root@master myspark]# ll
합계 4
-rw-r--r--. 1 root root 3465  8월  8 14:43 wc7.jar
[root@master myspark]# spark-submit --class sparkexam.WordCount7 wc8.jar
19/08/08 15:09:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/08/08 15:09:01 WARN SparkSubmit$$anon$2: Failed to load sparkexam.WordCount7.
java.lang.ClassNotFoundException: sparkexam.WordCount7
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:238)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:810)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[root@master myspark]# spark-submit --class sparkexam.WordCount8 wc8.jar
19/08/08 15:09:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[(banana,8), (pear,5), (orange,6), (grape,2), (cherry,8), (tomato,5), (,4), (peach,8), (apple,6)]
[root@master myspark]# ls
wc7.jar  wc8.jar
[root@master myspark]# hdfs dfs -ls /myspark
ls: `/myspark': No such file or directory
[root@master myspark]# hdfs dfs -ls /edudata
Found 8 items
-rw-r--r--   3 root    supergroup  689413344 2019-08-01 10:19 /edudata/2008.csv
-rw-r--r--   3 root    supergroup        314 2019-08-02 13:41 /edudata/fruit.txt
-rw-r--r--   3 student supergroup         68 2019-08-01 16:15 /edudata/message.txt
-rw-r--r--   3 student supergroup         47 2019-08-06 13:52 /edudata/null
-rw-r--r--   3 root    supergroup       7402 2019-08-01 10:19 /edudata/president_moon.txt
-rw-r--r--   3 root    supergroup      14174 2019-08-01 14:15 /edudata/product_click.log
-rw-r--r--   3 root    supergroup         14 2019-08-01 14:24 /edudata/sample.txt
-rw-r--r--   3 student supergroup     254370 2019-08-02 09:43 /edudata/tomcat_access_log.txt
[root@master myspark]# hdfs dfs -ls /output
Found 11 items
drwxr-xr-x   - root supergroup          0 2019-08-02 16:54 /output/exer_out1
drwxr-xr-x   - root supergroup          0 2019-08-02 17:33 /output/exer_out2
drwxr-xr-x   - root supergroup          0 2019-08-05 10:34 /output/exer_out3
drwxr-xr-x   - root supergroup          0 2019-08-02 16:23 /output/result
drwxr-xr-x   - root supergroup          0 2019-08-08 11:42 /output/scalaresult
drwxr-xr-x   - root supergroup          0 2019-08-08 14:47 /output/sparkoutput7
drwxr-xr-x   - root supergroup          0 2019-08-08 15:09 /output/sparkoutput8
drwxr-xr-x   - root supergroup          0 2019-08-02 13:29 /output/test1
drwxr-xr-x   - root supergroup          0 2019-08-02 13:43 /output/test2
drwxr-xr-x   - root supergroup          0 2019-08-02 14:21 /output/test2008
drwxr-xr-x   - root supergroup          0 2019-08-02 14:24 /output/test2008_1
[root@master myspark]# hdfs dfs -ls /output/sparkoutput7
Found 3 items
-rw-r--r--   3 root supergroup          0 2019-08-08 14:47 /output/sparkoutput7/_SUCCESS
-rw-r--r--   3 root supergroup         46 2019-08-08 14:47 /output/sparkoutput7/part-00000
-rw-r--r--   3 root supergroup         42 2019-08-08 14:47 /output/sparkoutput7/part-00001
[root@master myspark]# hdfs dfs -cat /output/sparkoutput7/part-00000
(pear,5)
(orange,6)
(tomato,5)
(,4)
(apple,6)
[root@master myspark]# ls
wc7.jar  wc8.jar
[root@master myspark]# hdfs dfs -ls /output/sparkoutput8
Found 2 items
-rw-r--r--   3 root supergroup          0 2019-08-08 15:09 /output/sparkoutput8/_SUCCESS
-rw-r--r--   3 root supergroup         88 2019-08-08 15:09 /output/sparkoutput8/part-00000
[root@master myspark]# hdfs dfs -cat /output/sparkoutput8/part-00000
(banana,8)
(pear,5)
(orange,6)
(grape,2)
(cherry,8)
(tomato,5)
(,4)
(peach,8)
(apple,6)
[root@master myspark]# hdfs dfs -rm -r /output/sparkoutput7
19/08/08 15:54:13 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted /output/sparkoutput7
[root@master myspark]# hdfs dfs -ls /output
Found 10 items
drwxr-xr-x   - root supergroup          0 2019-08-02 16:54 /output/exer_out1
drwxr-xr-x   - root supergroup          0 2019-08-02 17:33 /output/exer_out2
drwxr-xr-x   - root supergroup          0 2019-08-05 10:34 /output/exer_out3
drwxr-xr-x   - root supergroup          0 2019-08-02 16:23 /output/result
drwxr-xr-x   - root supergroup          0 2019-08-08 11:42 /output/scalaresult
drwxr-xr-x   - root supergroup          0 2019-08-08 15:09 /output/sparkoutput8
drwxr-xr-x   - root supergroup          0 2019-08-02 13:29 /output/test1
drwxr-xr-x   - root supergroup          0 2019-08-02 13:43 /output/test2
drwxr-xr-x   - root supergroup          0 2019-08-02 14:21 /output/test2008
drwxr-xr-x   - root supergroup          0 2019-08-02 14:24 /output/test2008_1
[root@master myspark]# hdfs dfs -rm -r /output/sparkoutput8
19/08/08 15:54:38 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted /output/sparkoutput8
[root@master myspark]# spark-submit --class sparkexam.WordCount7 wc7.jar
19/08/08 15:55:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/08/08 15:55:58 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
19/08/08 15:56:46 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[(pear,5), (orange,6), (tomato,5), (apple,6), (banana,8), (grape,2), (cherry,8), (peach,8)]
19/08/08 15:57:01 WARN NioEventLoop: Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@678bb3fb.
19/08/08 15:57:01 WARN NioEventLoop: Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@22b50559.
[root@master myspark]# hdfs dfs -ls /output
Found 10 items
drwxr-xr-x   - root supergroup          0 2019-08-02 16:54 /output/exer_out1
drwxr-xr-x   - root supergroup          0 2019-08-02 17:33 /output/exer_out2
drwxr-xr-x   - root supergroup          0 2019-08-05 10:34 /output/exer_out3
drwxr-xr-x   - root supergroup          0 2019-08-02 16:23 /output/result
drwxr-xr-x   - root supergroup          0 2019-08-08 11:42 /output/scalaresult
drwxr-xr-x   - root supergroup          0 2019-08-08 15:57 /output/sparkoutput7
drwxr-xr-x   - root supergroup          0 2019-08-02 13:29 /output/test1
drwxr-xr-x   - root supergroup          0 2019-08-02 13:43 /output/test2
drwxr-xr-x   - root supergroup          0 2019-08-02 14:21 /output/test2008
drwxr-xr-x   - root supergroup          0 2019-08-02 14:24 /output/test2008_1
[root@master myspark]# hdfs dfs -cat /output/sparkoutput7/part-00000
(pear,5)
(orange,6)
(tomato,5)
(apple,6)
[root@master myspark]# spark-submit --class sparkexam.WordCount8 wc8.jar
19/08/08 15:58:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[(banana,8), (pear,5), (orange,6), (grape,2), (cherry,8), (tomato,5), (peach,8), (apple,6)]
[root@master myspark]# hdfs dfs -cat /output/sparkoutput8/part-00000
(banana,8)
(pear,5)
(orange,6)
(grape,2)
(cherry,8)
(tomato,5)
(peach,8)
(apple,6)
[root@master myspark]# spark-submit --class sparkexam.RDDCreate rdd.jar
19/08/08 16:22:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
a
b
c
d
e
apple banana cherry peach
banana cherry peach  cherry  cherry
apple cherry peach grape
apple grape pear cherry peach
orange banana cherry orange
apple banana pear banana peach banana
orange banana orange peach orange orange
apple banana pear  pear  pear peach
apple tomato cherry peach
tomato tomato tomato tomato
[root@master myspark]# hdfs dfs -ls /output
Found 11 items
drwxr-xr-x   - root supergroup          0 2019-08-02 16:54 /output/exer_out1
drwxr-xr-x   - root supergroup          0 2019-08-02 17:33 /output/exer_out2
drwxr-xr-x   - root supergroup          0 2019-08-05 10:34 /output/exer_out3
drwxr-xr-x   - root supergroup          0 2019-08-02 16:23 /output/result
drwxr-xr-x   - root supergroup          0 2019-08-08 11:42 /output/scalaresult
drwxr-xr-x   - root supergroup          0 2019-08-08 15:57 /output/sparkoutput7
drwxr-xr-x   - root supergroup          0 2019-08-08 15:58 /output/sparkoutput8
drwxr-xr-x   - root supergroup          0 2019-08-02 13:29 /output/test1
drwxr-xr-x   - root supergroup          0 2019-08-02 13:43 /output/test2
drwxr-xr-x   - root supergroup          0 2019-08-02 14:21 /output/test2008
drwxr-xr-x   - root supergroup          0 2019-08-02 14:24 /output/test2008_1
[root@master myspark]# cd
[root@master ~]# hdfs dfs -mkdir /spark-logs
[root@master ~]# hdfs dfs -chmod 777 /spark-logs
[root@master ~]# cd $SPARK_HOME
[root@master spark-2.4.3]# ls
LICENSE  R          RELEASE  conf  examples  kubernetes  myspark  sbin
NOTICE   README.md  bin      data  jars      licenses    python   yarn
[root@master spark-2.4.3]# cd conf
[root@master conf]# pwd
/root/spark-2.4.3/conf
[root@master conf]# cp spark-defaults.conf.template spark-defaults.conf
[root@master conf]# vi spark-defaults.conf
[root@master conf]# cat spark-defaults.conf
#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Example:
# spark.master                     spark://master:7077
# spark.eventLog.enabled           true
# spark.eventLog.dir               hdfs://namenode:8021/directory
# spark.serializer                 org.apache.spark.serializer.KryoSerializer
# spark.driver.memory              5g
# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
spark.master    yarn
spark.driver.memory    512m
spark.yarn.am.memory    512m
spark.executor.memory          512m
spark.eventLog.enabled  true
spark.eventLog.dir hdfs://192.168.111.120:9000/spark-logs
spark.history.provider            org.apache.spark.deploy.history.FsHistoryProvider
spark.history.fs.logDirectory     hdfs://192.168.111.120:9000/spark-logs
spark.history.fs.update.interval  10s
spark.history.ui.port             18080

[root@master conf]# start-history-server.sh
starting org.apache.spark.deploy.history.HistoryServer, logging to /root/spark-2.4.3/logs/spark-root-org.apache.spark.deploy.history.HistoryServer-1-master.out
[root@master conf]# hdfs dfs -rm -r /output/spark*
19/08/08 16:40:08 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted /output/sparkoutput7
19/08/08 16:40:08 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted /output/sparkoutput8
[root@master conf]# spark-submit --class sparkexam.WordCount8 wc8.jar
19/08/08 16:41:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/08/08 16:41:27 WARN DependencyUtils: Local jar /root/spark-2.4.3/conf/wc8.jar does not exist, skipping.
19/08/08 16:41:27 WARN SparkSubmit$$anon$2: Failed to load sparkexam.WordCount8.
java.lang.ClassNotFoundException: sparkexam.WordCount8
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:238)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:810)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[root@master conf]# cd
[root@master ~]# cd $SPARK_HOME
[root@master spark-2.4.3]# ls
LICENSE  R          RELEASE  conf  examples  kubernetes  logs     python  yarn
NOTICE   README.md  bin      data  jars      licenses    myspark  sbin
[root@master spark-2.4.3]# cd myspark
[root@master myspark]# spark-submit --class sparkexam.WordCount7 wc7.jar
19/08/08 16:43:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/08/08 16:43:05 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
[(pear,5), (orange,6), (tomato,5), (apple,6), (banana,8), (grape,2), (cherry,8), (peach,8)]
[root@master myspark]# spark-submit --class sparkexam.WordCount8 wc8.jar
19/08/08 16:44:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[(banana,8), (pear,5), (orange,6), (grape,2), (cherry,8), (tomato,5), (peach,8), (apple,6)]
[root@master myspark]# 

```

