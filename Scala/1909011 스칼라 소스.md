```scala
[root@master ~]# spark-shell
19/09/11 14:18:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[root@master ~]# start-dfs.sh
Starting namenodes on [master]
master: starting namenode, logging to /root/hadoop-2.7.7/logs/hadoop-root-namenode-master.out
master: starting datanode, logging to /root/hadoop-2.7.7/logs/hadoop-root-datanode-master.out
Starting secondary namenodes [master]
master: starting secondarynamenode, logging to /root/hadoop-2.7.7/logs/hadoop-root-secondarynamenode-master.out
[root@master ~]# start-yarn.sh
starting yarn daemons
starting resourcemanager, logging to /root/hadoop-2.7.7/logs/yarn-root-resourcemanager-master.out
master: starting nodemanager, logging to /root/hadoop-2.7.7/logs/yarn-root-nodemanager-master.out
[root@master ~]# spark-shell
19/09/11 14:22:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/09/11 14:22:57 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
19/09/11 14:23:21 ERROR SparkContext: Error initializing SparkContext.
java.io.FileNotFoundException: File does not exist: hdfs://192.168.111.120:9000/spark-logs
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1309)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1317)
	at org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:97)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:523)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:935)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:926)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:926)
	at org.apache.spark.repl.Main$.createSparkSession(Main.scala:106)
	at $line3.$read$$iw$$iw.<init>(<console>:15)
	at $line3.$read$$iw.<init>(<console>:43)
	at $line3.$read.<init>(<console>:45)
	at $line3.$read$.<init>(<console>:49)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.$print$lzycompute(<console>:7)
	at $line3.$eval$.$print(<console>:6)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)
	at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)
	at scala.tools.nsc.interpreter.IMain$$anonfun$quietRun$1.apply(IMain.scala:231)
	at scala.tools.nsc.interpreter.IMain$$anonfun$quietRun$1.apply(IMain.scala:231)
	at scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:221)
	at scala.tools.nsc.interpreter.IMain.quietRun(IMain.scala:231)
	at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1$$anonfun$apply$mcV$sp$1.apply(SparkILoop.scala:109)
	at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1$$anonfun$apply$mcV$sp$1.apply(SparkILoop.scala:109)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply$mcV$sp(SparkILoop.scala:109)
	at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply(SparkILoop.scala:109)
	at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply(SparkILoop.scala:109)
	at scala.tools.nsc.interpreter.ILoop.savingReplayStack(ILoop.scala:91)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:108)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$org$apache$spark$repl$SparkILoop$$anonfun$$loopPostInit$1$1.apply$mcV$sp(SparkILoop.scala:211)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$org$apache$spark$repl$SparkILoop$$anonfun$$loopPostInit$1$1.apply(SparkILoop.scala:199)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$org$apache$spark$repl$SparkILoop$$anonfun$$loopPostInit$1$1.apply(SparkILoop.scala:199)
	at scala.tools.nsc.interpreter.ILoop$$anonfun$mumly$1.apply(ILoop.scala:189)
	at scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:221)
	at scala.tools.nsc.interpreter.ILoop.mumly(ILoop.scala:186)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.org$apache$spark$repl$SparkILoop$$anonfun$$loopPostInit$1(SparkILoop.scala:199)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$startup$1$1.apply(SparkILoop.scala:267)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$startup$1$1.apply(SparkILoop.scala:247)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.withSuppressedSettings$1(SparkILoop.scala:235)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.startup$1(SparkILoop.scala:247)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:282)
	at org.apache.spark.repl.SparkILoop.runClosure(SparkILoop.scala:159)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:182)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
19/09/11 14:23:22 ERROR Main: Failed to initialize Spark session.
java.io.FileNotFoundException: File does not exist: hdfs://192.168.111.120:9000/spark-logs
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1309)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1317)
	at org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:97)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:523)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:935)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:926)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:926)
	at org.apache.spark.repl.Main$.createSparkSession(Main.scala:106)
	at $line3.$read$$iw$$iw.<init>(<console>:15)
	at $line3.$read$$iw.<init>(<console>:43)
	at $line3.$read.<init>(<console>:45)
	at $line3.$read$.<init>(<console>:49)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.$print$lzycompute(<console>:7)
	at $line3.$eval$.$print(<console>:6)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)
	at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)
	at scala.tools.nsc.interpreter.IMain$$anonfun$quietRun$1.apply(IMain.scala:231)
	at scala.tools.nsc.interpreter.IMain$$anonfun$quietRun$1.apply(IMain.scala:231)
	at scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:221)
	at scala.tools.nsc.interpreter.IMain.quietRun(IMain.scala:231)
	at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1$$anonfun$apply$mcV$sp$1.apply(SparkILoop.scala:109)
	at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1$$anonfun$apply$mcV$sp$1.apply(SparkILoop.scala:109)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply$mcV$sp(SparkILoop.scala:109)
	at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply(SparkILoop.scala:109)
	at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply(SparkILoop.scala:109)
	at scala.tools.nsc.interpreter.ILoop.savingReplayStack(ILoop.scala:91)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:108)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$org$apache$spark$repl$SparkILoop$$anonfun$$loopPostInit$1$1.apply$mcV$sp(SparkILoop.scala:211)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$org$apache$spark$repl$SparkILoop$$anonfun$$loopPostInit$1$1.apply(SparkILoop.scala:199)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$org$apache$spark$repl$SparkILoop$$anonfun$$loopPostInit$1$1.apply(SparkILoop.scala:199)
	at scala.tools.nsc.interpreter.ILoop$$anonfun$mumly$1.apply(ILoop.scala:189)
	at scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:221)
	at scala.tools.nsc.interpreter.ILoop.mumly(ILoop.scala:186)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.org$apache$spark$repl$SparkILoop$$anonfun$$loopPostInit$1(SparkILoop.scala:199)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$startup$1$1.apply(SparkILoop.scala:267)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$startup$1$1.apply(SparkILoop.scala:247)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.withSuppressedSettings$1(SparkILoop.scala:235)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.startup$1(SparkILoop.scala:247)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:282)
	at org.apache.spark.repl.SparkILoop.runClosure(SparkILoop.scala:159)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:182)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[root@master ~]# spark-shell
19/09/11 14:24:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/09/11 14:24:18 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
19/09/11 14:24:33 ERROR SparkContext: Error initializing SparkContext.
java.io.FileNotFoundException: File does not exist: hdfs://192.168.111.120:9000/spark-logs
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1309)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1317)
	at org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:97)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:523)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:935)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:926)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:926)
	at org.apache.spark.repl.Main$.createSparkSession(Main.scala:106)
	at $line3.$read$$iw$$iw.<init>(<console>:15)
	at $line3.$read$$iw.<init>(<console>:43)
	at $line3.$read.<init>(<console>:45)
	at $line3.$read$.<init>(<console>:49)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.$print$lzycompute(<console>:7)
	at $line3.$eval$.$print(<console>:6)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)
	at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)
	at scala.tools.nsc.interpreter.IMain$$anonfun$quietRun$1.apply(IMain.scala:231)
	at scala.tools.nsc.interpreter.IMain$$anonfun$quietRun$1.apply(IMain.scala:231)
	at scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:221)
	at scala.tools.nsc.interpreter.IMain.quietRun(IMain.scala:231)
	at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1$$anonfun$apply$mcV$sp$1.apply(SparkILoop.scala:109)
	at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1$$anonfun$apply$mcV$sp$1.apply(SparkILoop.scala:109)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply$mcV$sp(SparkILoop.scala:109)
	at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply(SparkILoop.scala:109)
	at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply(SparkILoop.scala:109)
	at scala.tools.nsc.interpreter.ILoop.savingReplayStack(ILoop.scala:91)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:108)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$org$apache$spark$repl$SparkILoop$$anonfun$$loopPostInit$1$1.apply$mcV$sp(SparkILoop.scala:211)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$org$apache$spark$repl$SparkILoop$$anonfun$$loopPostInit$1$1.apply(SparkILoop.scala:199)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$org$apache$spark$repl$SparkILoop$$anonfun$$loopPostInit$1$1.apply(SparkILoop.scala:199)
	at scala.tools.nsc.interpreter.ILoop$$anonfun$mumly$1.apply(ILoop.scala:189)
	at scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:221)
	at scala.tools.nsc.interpreter.ILoop.mumly(ILoop.scala:186)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.org$apache$spark$repl$SparkILoop$$anonfun$$loopPostInit$1(SparkILoop.scala:199)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$startup$1$1.apply(SparkILoop.scala:267)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$startup$1$1.apply(SparkILoop.scala:247)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.withSuppressedSettings$1(SparkILoop.scala:235)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.startup$1(SparkILoop.scala:247)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:282)
	at org.apache.spark.repl.SparkILoop.runClosure(SparkILoop.scala:159)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:182)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
19/09/11 14:24:33 ERROR Main: Failed to initialize Spark session.
java.io.FileNotFoundException: File does not exist: hdfs://192.168.111.120:9000/spark-logs
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1309)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1317)
	at org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:97)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:523)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:935)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:926)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:926)
	at org.apache.spark.repl.Main$.createSparkSession(Main.scala:106)
	at $line3.$read$$iw$$iw.<init>(<console>:15)
	at $line3.$read$$iw.<init>(<console>:43)
	at $line3.$read.<init>(<console>:45)
	at $line3.$read$.<init>(<console>:49)
	at $line3.$read$.<clinit>(<console>)
	at $line3.$eval$.$print$lzycompute(<console>:7)
	at $line3.$eval$.$print(<console>:6)
	at $line3.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)
	at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)
	at scala.tools.nsc.interpreter.IMain$$anonfun$quietRun$1.apply(IMain.scala:231)
	at scala.tools.nsc.interpreter.IMain$$anonfun$quietRun$1.apply(IMain.scala:231)
	at scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:221)
	at scala.tools.nsc.interpreter.IMain.quietRun(IMain.scala:231)
	at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1$$anonfun$apply$mcV$sp$1.apply(SparkILoop.scala:109)
	at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1$$anonfun$apply$mcV$sp$1.apply(SparkILoop.scala:109)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply$mcV$sp(SparkILoop.scala:109)
	at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply(SparkILoop.scala:109)
	at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply(SparkILoop.scala:109)
	at scala.tools.nsc.interpreter.ILoop.savingReplayStack(ILoop.scala:91)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:108)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$org$apache$spark$repl$SparkILoop$$anonfun$$loopPostInit$1$1.apply$mcV$sp(SparkILoop.scala:211)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$org$apache$spark$repl$SparkILoop$$anonfun$$loopPostInit$1$1.apply(SparkILoop.scala:199)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$org$apache$spark$repl$SparkILoop$$anonfun$$loopPostInit$1$1.apply(SparkILoop.scala:199)
	at scala.tools.nsc.interpreter.ILoop$$anonfun$mumly$1.apply(ILoop.scala:189)
	at scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:221)
	at scala.tools.nsc.interpreter.ILoop.mumly(ILoop.scala:186)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.org$apache$spark$repl$SparkILoop$$anonfun$$loopPostInit$1(SparkILoop.scala:199)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$startup$1$1.apply(SparkILoop.scala:267)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1$$anonfun$startup$1$1.apply(SparkILoop.scala:247)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.withSuppressedSettings$1(SparkILoop.scala:235)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.startup$1(SparkILoop.scala:247)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:282)
	at org.apache.spark.repl.SparkILoop.runClosure(SparkILoop.scala:159)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:182)
	at org.apache.spark.repl.Main$.doMain(Main.scala:78)
	at org.apache.spark.repl.Main$.main(Main.scala:58)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[root@master ~]# jps
6737 Jps
5187 DataNode
5636 NodeManager
5516 ResourceManager
5054 NameNode
5359 SecondaryNameNode
[root@master ~]# cd sampledata
[root@master sampledata]# cd
[root@master ~]# cd sampledata
[root@master sampledata]# echo $PATH
/root/apache-hive/bin:/root/spark-2.4.3/bin:/root/spark-2.4.3/sbin:/root/hadoop-2.7.7/bin:/root/hadoop-2.7.7/sbin:/root/apache-hive/bin:/root/spark-2.4.3/bin:/root/spark-2.4.3/sbin:/root/hadoop-2.7.7/bin:/root/hadoop-2.7.7/sbin:/usr/local/java/bin:/usr/local/bin:/usr/local/sbin:/usr/bin:/usr/sbin:/bin:/sbin:/root/bin
[root@master sampledata]# cd $SPARK_HOME/sbin
[root@master sbin]# ls
slaves.sh                       start-slaves.sh
spark-config.sh                 start-thriftserver.sh
spark-daemon.sh                 stop-all.sh
spark-daemons.sh                stop-history-server.sh
start-all.sh                    stop-master.sh
start-history-server.sh         stop-mesos-dispatcher.sh
start-master.sh                 stop-mesos-shuffle-service.sh
start-mesos-dispatcher.sh       stop-shuffle-service.sh
start-mesos-shuffle-service.sh  stop-slave.sh
start-shuffle-service.sh        stop-slaves.sh
start-slave.sh                  stop-thriftserver.sh
[root@master sbin]# cd ../bin
[root@master bin]# ls
beeline               pyspark          spark-class2.cmd  spark-submit
beeline.cmd           pyspark.cmd      spark-shell       spark-submit.cmd
docker-image-tool.sh  pyspark2.cmd     spark-shell.cmd   spark-submit2.cmd
find-spark-home       run-example      spark-shell2.cmd  sparkR
find-spark-home.cmd   run-example.cmd  spark-sql         sparkR.cmd
load-spark-env.cmd    spark-class      spark-sql.cmd     sparkR2.cmd
load-spark-env.sh     spark-class.cmd  spark-sql2.cmd

[root@master bin]# cd
[root@master ~]# hdfs dfs -mkdir /spark-logs
[root@master ~]# spark-shell
19/09/11 14:32:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/09/11 14:32:49 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
Spark context Web UI available at http://master:4040
Spark context available as 'sc' (master = yarn, app id = application_1568179343812_0004).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.3
      /_/
         
Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_131)
Type in expressions to have them evaluated.
Type :help for more information.

scala> sc
res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@a8177f6

scala> val x = sc.parallelize(List("spark","rdd","example","sample","example"))
x: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:24

scala> x.count
res1: Long = 5                                                                  

scala> x.first
res2: String = spark

scala> val y = x.map((_,1))
y: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[1] at map at <console>:25

scala> y.collect
res3: Array[(String, Int)] = Array((spark,1), (rdd,1), (example,1), (sample,1), (example,1))

scala> y.collect.foreach(println)
(spark,1)
(rdd,1)
(example,1)
(sample,1)
(example,1)

scala> val y = x.map(x => (x,x.length))
y: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[2] at map at <console>:25

scala> y.collect
res5: Array[(String, Int)] = Array((spark,5), (rdd,3), (example,7), (sample,6), (example,7))

scala> y.collect.foreach(println)
(spark,5)
(rdd,3)
(example,7)
(sample,6)
(example,7)

scala> val x = sc.parallelize(List("spark rdd example", "sample example"))
x: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[3] at parallelize at <console>:24

scala> val y = x.map(x => x.split(" ")) 
y: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[4] at map at <console>:25

scala> y.collect
res7: Array[Array[String]] = Array(Array(spark, rdd, example), Array(sample, example))

scala> y.collect.foreach(_.foreach(println))
spark
rdd
example
sample
example

scala> val y = x.flatMap(_.split(" "))
y: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[5] at flatMap at <console>:25

scala> y.collect
res9: Array[String] = Array(spark, rdd, example, sample, example)

scala> y.collect.foreach(println)
spark
rdd
example
sample
example

scala> val x = sc.parallelize(1 to 10)
x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at parallelize at <console>:24

scala> val y = x.filter(e => ex e % 2==0)
<console>:1: error: ')' expected but integer literal found.
val y = x.filter(e => ex e % 2==0)
                             ^

scala> val y = x.filter(e => ex e%2==0)
<console>:1: error: ')' expected but integer literal found.
val y = x.filter(e => ex e%2==0)
                           ^

scala> val y = x.filter(e => e % 2==0)
y: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[7] at filter at <console>:25

scala> y.collect
res11: Array[Int] = Array(2, 4, 6, 8, 10)

scala> y.collect.foreach(println)
2
4
6
8
10

scala> val y = x.filter(_ % 2 ==0)
y: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[8] at filter at <console>:25

scala> y.collect
res13: Array[Int] = Array(2, 4, 6, 8, 10)

scala> y.collect.foreach(println)
2
4
6
8
10

scala> val x = sc.parallelize(1 to 10,2 (
     | [root@master ~]# spark-shell
19/09/11 15:09:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/09/11 15:09:17 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
Spark context Web UI available at http://master:4040
Spark context available as 'sc' (master = yarn, app id = application_1568179343812_0005).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.3
      /_/
         
Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_131)
Type in expressions to have them evaluated.
Type :help for more information.

scala> val x = sc.parallelize(1 to 10,2 )
x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24

scala> val y = x.reduce((result,n) => (result + n))
y: Int = 55                                                                     

scala> val y = x.reduce(_ + _)
y: Int = 55

scala> val y = x.reduce(_ * _)
y: Int = 3628800

scala> val x = sc.parallelize(Array(("a",1),("b",1),("a",1),
     | ("a",1),("b",1),("b",1),("b",1),("b",1)))
x: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[1] at parallelize at <console>:24

scala> val y = x.reduceByKey((result,n) => (result + n))
y: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[2] at reduceByKey at <console>:25

scala> y.collect
res0: Array[(String, Int)] = Array((b,5), (a,3))

scala> y.collect.foreach(println)
(b,5)
(a,3)

scala> def sumFunc(result:Int, n:Int) = result + n
sumFunc: (result: Int, n: Int)Int

scala> val y = x.reduceByKey(sumFunc)
y: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[3] at reduceByKey at <console>:27

scala> y.collect
res2: Array[(String, Int)] = Array((b,5), (a,3))

scala> y.collect.foreach(println)
(b,5)
(a,3)

scala> val x = sc.parallelize(Array("Joseph","Jimmy","Tina",
     | "Thomas","James","Cory","Christine","Jackeline","Juan"),3)
x: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[4] at parallelize at <console>:24

scala> val y = x.groupBy(word => word.charAt(0))
y: org.apache.spark.rdd.RDD[(Char, Iterable[String])] = ShuffledRDD[6] at groupBy at <console>:25

scala> y.collect
res4: Array[(Char, Iterable[String])] = Array((T,CompactBuffer(Tina, Thomas)), (C,CompactBuffer(Cory, Christine)), (J,CompactBuffer(Joseph, Jimmy, James, Jackeline, Juan)))

scala> y.collect.foreach(println)
(T,CompactBuffer(Tina, Thomas))
(C,CompactBuffer(Cory, Christine))
(J,CompactBuffer(Joseph, Jimmy, James, Jackeline, Juan))

scala> val y = x.groupBy(_.chartAt(0))
<console>:25: error: value chartAt is not a member of String
       val y = x.groupBy(_.chartAt(0))
                           ^

scala> val y = x.groupBy(_.charAt(0))
y: org.apache.spark.rdd.RDD[(Char, Iterable[String])] = ShuffledRDD[8] at groupBy at <console>:25

scala> y.collect
res6: Array[(Char, Iterable[String])] = Array((T,CompactBuffer(Tina, Thomas)), (C,CompactBuffer(Cory, Christine)), (J,CompactBuffer(Joseph, Jimmy, James, Jackeline, Juan)))

scala> y.collect.foreach(println)
(T,CompactBuffer(Tina, Thomas))
(C,CompactBuffer(Cory, Christine))
(J,CompactBuffer(Joseph, Jimmy, James, Jackeline, Juan))

scala> val y = x.groupBy(_.length)
y: org.apache.spark.rdd.RDD[(Int, Iterable[String])] = ShuffledRDD[10] at groupBy at <console>:25

scala> y.collect
res8: Array[(Int, Iterable[String])] = Array((6,CompactBuffer(Joseph, Thomas)), (9,CompactBuffer(Christine, Jackeline)), (4,CompactBuffer(Tina, Cory, Juan)), (5,CompactBuffer(Jimmy, James)))

scala> y.collect.foreach(println)
(6,CompactBuffer(Joseph, Thomas))
(9,CompactBuffer(Christine, Jackeline))
(4,CompactBuffer(Tina, Cory, Juan))
(5,CompactBuffer(Jimmy, James))

scala> val textRDD = sc.textFile("/path/to/simple-words.txt")
textRDD: org.apache.spark.rdd.RDD[String] = /path/to/simple-words.txt MapPartitionsRDD[12] at textFile at <console>:24

scala> val textArray = textRDD.collect
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://master:9000/path/to/simple-words.txt
  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
  ... 49 elided

scala> val textRDD = sc.textFile("/path/to/simple-words.txt")
textRDD: org.apache.spark.rdd.RDD[String] = /path/to/simple-words.txt MapPartitionsRDD[14] at textFile at <console>:24

scala> val textArray = textRDD.collect
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://master:9000/path/to/simple-words.txt
  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
  ... 49 elided

scala> "file:///root/sampledata/simple-words.txt"
res10: String = file:///root/sampledata/simple-words.txt

scala> val textArray = textRDD.collect
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://master:9000/path/to/simple-words.txt
  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
  ... 49 elided

scala> val textRDD = sc.textFile("file:///root/sampledata/simple-words.txt")
textRDD: org.apache.spark.rdd.RDD[String] = file:///root/sampledata/simple-words.txt MapPartitionsRDD[16] at textFile at <console>:24

scala> val textArray = textRDD.collect
textArray: Array[String] = Array(cat, dog, .org, cat, cat, &&, tiger, dog, 100, tiger, cat)

scala> textArray.foreach(println)
cat
dog
.org
cat
cat
&&
tiger
dog
100
tiger
cat

scala> val wordArray = wordRDD.collect
<console>:23: error: not found: value wordRDD
       val wordArray = wordRDD.collect
                       ^

scala> val isWord: String => Boolean = word => word.matches("""\p{Alnum}+""")
isWord: String => Boolean = <function1>

scala> val wordRDD = textRDD.filter(isWord)
wordRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[17] at filter at <console>:27

scala> val wordArray = wordRDD.collect
wordArray: Array[String] = Array(cat, dog, cat, cat, tiger, dog, 100, tiger, cat)

scala> wordArray.foreach(println)
cat
dog
cat
cat
tiger
dog
100
tiger
cat

scala> val wordAndOnePairRDD = wordRdd.map(word => (word,1))
<console>:23: error: not found: value wordRdd
       val wordAndOnePairRDD = wordRdd.map(word => (word,1))
                               ^

scala> val wordAndOnePairRDD = wordRDD.map(word => (word,1))
wordAndOnePairRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[18] at map at <console>:25

scala> val wordAndOnePairArray = wordAndOnePairRdd.collect
<console>:23: error: not found: value wordAndOnePairRdd
       val wordAndOnePairArray = wordAndOnePairRdd.collect
                                 ^

scala> val wordAndOnePairArray = wordAndOnePairRDD.collect
wordAndOnePairArray: Array[(String, Int)] = Array((cat,1), (dog,1), (cat,1), (cat,1), (tiger,1), (dog,1), (100,1), (tiger,1), (cat,1))

scala> wordAndOnePairArray.foreach(println)
(cat,1)
(dog,1)
(cat,1)
(cat,1)
(tiger,1)
(dog,1)
(100,1)
(tiger,1)
(cat,1)

scala> val wordAndCountRDD = wordAndOnePairRDD.reduceByKey((result, elem) => result + elem)
wordAndCountRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[19] at reduceByKey at <console>:25

scala> val wordAndCOuntArray = wordAndCountRDD.collect
wordAndCOuntArray: Array[(String, Int)] = Array((dog,2), (cat,4), (tiger,2), (100,1))

scala> wordAndCountArray.foreach(prinln)
<console>:24: error: not found: value wordAndCountArray
       wordAndCountArray.foreach(prinln)
       ^
<console>:24: error: not found: value prinln
       wordAndCountArray.foreach(prinln)
                                 ^

scala> wordAndCountArray.foreach(println)
<console>:24: error: not found: value wordAndCountArray
       wordAndCountArray.foreach(println)
       ^

scala> wordAndCountArray.foreach(println)
<console>:24: error: not found: value wordAndCountArray
       wordAndCountArray.foreach(println)
       ^

scala> val wordAndCountArray = wordAndCountRDD.collect
wordAndCountArray: Array[(String, Int)] = Array((dog,2), (cat,4), (tiger,2), (100,1))

scala> wordAndCountArray.foreach(println)
(dog,2)
(cat,4)
(tiger,2)
(100,1)

scala> val textRDD = sc.textFile("file:///root/sampledata/README.md")
textRDD: org.apache.spark.rdd.RDD[String] = file:///root/sampledata/README.md MapPartitionsRDD[21] at textFile at <console>:24

scala> val textArray = textRDD.collect
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/root/sampledata/README.md
  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
  ... 49 elided

scala> val textRDD = sc.textFile("file:///root/spark-2.4.3/README.md")
textRDD: org.apache.spark.rdd.RDD[String] = file:///root/spark-2.4.3/README.md MapPartitionsRDD[23] at textFile at <console>:24

scala> val textArray = textRDD.collect
textArray: Array[String] = Array(# Apache Spark, "", Spark is a fast and general cluster computing system for Big Data. It provides, high-level APIs in Scala, Java, Python, and R, and an optimized engine that, supports general computation graphs for data analysis. It also supports a, rich set of higher-level tools including Spark SQL for SQL and DataFrames,, MLlib for machine learning, GraphX for graph processing,, and Spark Streaming for stream processing., "", <http://spark.apache.org/>, "", "", ## Online Documentation, "", You can find the latest Spark documentation, including a programming, guide, on the [project web page](http://spark.apache.org/documentation.html)., This README file only contains basic setup instructions., "", ## Building Spark, "", Spark is built using [Apache Ma...
scala> textArray.foreach(println)
# Apache Spark

Spark is a fast and general cluster computing system for Big Data. It provides
high-level APIs in Scala, Java, Python, and R, and an optimized engine that
supports general computation graphs for data analysis. It also supports a
rich set of higher-level tools including Spark SQL for SQL and DataFrames,
MLlib for machine learning, GraphX for graph processing,
and Spark Streaming for stream processing.

<http://spark.apache.org/>


## Online Documentation

You can find the latest Spark documentation, including a programming
guide, on the [project web page](http://spark.apache.org/documentation.html).
This README file only contains basic setup instructions.

## Building Spark

Spark is built using [Apache Maven](http://maven.apache.org/).
To build Spark and its example programs, run:

    build/mvn -DskipTests clean package

(You do not need to do this if you downloaded a pre-built package.)

You can build Spark using more than one thread by using the -T option with Maven, see ["Parallel builds in Maven 3"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).
More detailed documentation is available from the project site, at
["Building Spark"](http://spark.apache.org/docs/latest/building-spark.html).

For general development tips, including info on developing Spark using an IDE, see ["Useful Developer Tools"](http://spark.apache.org/developer-tools.html).

## Interactive Scala Shell

The easiest way to start using Spark is through the Scala shell:

    ./bin/spark-shell

Try the following command, which should return 1000:

    scala> sc.parallelize(1 to 1000).count()

## Interactive Python Shell

Alternatively, if you prefer Python, you can use the Python shell:

    ./bin/pyspark

And run the following command, which should also return 1000:

    >>> sc.parallelize(range(1000)).count()

## Example Programs

Spark also comes with several sample programs in the `examples` directory.
To run one of them, use `./bin/run-example <class> [params]`. For example:

    ./bin/run-example SparkPi

will run the Pi example locally.

You can set the MASTER environment variable when running examples to submit
examples to a cluster. This can be a mesos:// or spark:// URL,
"yarn" to run on YARN, and "local" to run
locally with one thread, or "local[N]" to run locally with N threads. You
can also use an abbreviated class name if the class is in the `examples`
package. For instance:

    MASTER=spark://host:7077 ./bin/run-example SparkPi

Many of the example programs print usage help if no params are given.

## Running Tests

Testing first requires [building Spark](#building-spark). Once Spark is built, tests
can be run using:

    ./dev/run-tests

Please see the guidance on how to
[run tests for a module, or individual tests](http://spark.apache.org/developer-tools.html#individual-tests).

There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md

## A Note About Hadoop Versions

Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported
storage systems. Because the protocols have changed in different versions of
Hadoop, you must build Spark against the same version that your cluster runs.

Please refer to the build documentation at
["Specifying the Hadoop Version and Enabling YARN"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)
for detailed guidance on building for a particular distribution of Hadoop, including
building for particular Hive and Hive Thriftserver distributions.

## Configuration

Please refer to the [Configuration Guide](http://spark.apache.org/docs/latest/configuration.html)
in the online documentation for an overview on how to configure Spark.

## Contributing

Please review the [Contribution to Spark guide](http://spark.apache.org/contributing.html)
for information on how to get started contributing to the project.

scala> val wordCandidateRDD = textRDD.flatMap(_.split("[ ,.]"))
wordCandidateRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[24] at flatMap at <console>:25

scala> val wordCandidateArray = wordCandidateRDD.collect
wordCandidateArray: Array[String] = Array(#, Apache, Spark, "", Spark, is, a, fast, and, general, cluster, computing, system, for, Big, Data, "", It, provides, high-level, APIs, in, Scala, "", Java, "", Python, "", and, R, "", and, an, optimized, engine, that, supports, general, computation, graphs, for, data, analysis, "", It, also, supports, a, rich, set, of, higher-level, tools, including, Spark, SQL, for, SQL, and, DataFrames, MLlib, for, machine, learning, "", GraphX, for, graph, processing, and, Spark, Streaming, for, stream, processing, "", <http://spark, apache, org/>, "", "", ##, Online, Documentation, "", You, can, find, the, latest, Spark, documentation, "", including, a, programming, guide, "", on, the, [project, web, page](http://spark, apache, org/documentation, html), Thi...
scala> wordCandidateArray.foreach(println)
#
Apache
Spark

Spark
is
a
fast
and
general
cluster
computing
system
for
Big
Data

It
provides
high-level
APIs
in
Scala

Java

Python

and
R

and
an
optimized
engine
that
supports
general
computation
graphs
for
data
analysis

It
also
supports
a
rich
set
of
higher-level
tools
including
Spark
SQL
for
SQL
and
DataFrames
MLlib
for
machine
learning

GraphX
for
graph
processing
and
Spark
Streaming
for
stream
processing

<http://spark
apache
org/>


##
Online
Documentation

You
can
find
the
latest
Spark
documentation

including
a
programming
guide

on
the
[project
web
page](http://spark
apache
org/documentation
html)
This
README
file
only
contains
basic
setup
instructions

##
Building
Spark

Spark
is
built
using
[Apache
Maven](http://maven
apache
org/)
To
build
Spark
and
its
example
programs

run:





build/mvn
-DskipTests
clean
package

(You
do
not
need
to
do
this
if
you
downloaded
a
pre-built
package
)

You
can
build
Spark
using
more
than
one
thread
by
using
the
-T
option
with
Maven

see
["Parallel
builds
in
Maven
3"](https://cwiki
apache
org/confluence/display/MAVEN/Parallel+builds+in+Maven+3)
More
detailed
documentation
is
available
from
the
project
site

at
["Building
Spark"](http://spark
apache
org/docs/latest/building-spark
html)

For
general
development
tips

including
info
on
developing
Spark
using
an
IDE

see
["Useful
Developer
Tools"](http://spark
apache
org/developer-tools
html)

##
Interactive
Scala
Shell

The
easiest
way
to
start
using
Spark
is
through
the
Scala
shell:






/bin/spark-shell

Try
the
following
command

which
should
return
1000:





scala>
sc
parallelize(1
to
1000)
count()

##
Interactive
Python
Shell

Alternatively

if
you
prefer
Python

you
can
use
the
Python
shell:






/bin/pyspark

And
run
the
following
command

which
should
also
return
1000:





>>>
sc
parallelize(range(1000))
count()

##
Example
Programs

Spark
also
comes
with
several
sample
programs
in
the
`examples`
directory
To
run
one
of
them

use
`
/bin/run-example
<class>
[params]`

For
example:






/bin/run-example
SparkPi

will
run
the
Pi
example
locally

You
can
set
the
MASTER
environment
variable
when
running
examples
to
submit
examples
to
a
cluster

This
can
be
a
mesos://
or
spark://
URL
"yarn"
to
run
on
YARN

and
"local"
to
run
locally
with
one
thread

or
"local[N]"
to
run
locally
with
N
threads

You
can
also
use
an
abbreviated
class
name
if
the
class
is
in
the
`examples`
package

For
instance:





MASTER=spark://host:7077

/bin/run-example
SparkPi

Many
of
the
example
programs
print
usage
help
if
no
params
are
given

##
Running
Tests

Testing
first
requires
[building
Spark](#building-spark)

Once
Spark
is
built

tests
can
be
run
using:






/dev/run-tests

Please
see
the
guidance
on
how
to
[run
tests
for
a
module

or
individual
tests](http://spark
apache
org/developer-tools
html#individual-tests)

There
is
also
a
Kubernetes
integration
test

see
resource-managers/kubernetes/integration-tests/README
md

##
A
Note
About
Hadoop
Versions

Spark
uses
the
Hadoop
core
library
to
talk
to
HDFS
and
other
Hadoop-supported
storage
systems

Because
the
protocols
have
changed
in
different
versions
of
Hadoop

you
must
build
Spark
against
the
same
version
that
your
cluster
runs

Please
refer
to
the
build
documentation
at
["Specifying
the
Hadoop
Version
and
Enabling
YARN"](http://spark
apache
org/docs/latest/building-spark
html#specifying-the-hadoop-version-and-enabling-yarn)
for
detailed
guidance
on
building
for
a
particular
distribution
of
Hadoop

including
building
for
particular
Hive
and
Hive
Thriftserver
distributions

##
Configuration

Please
refer
to
the
[Configuration
Guide](http://spark
apache
org/docs/latest/configuration
html)
in
the
online
documentation
for
an
overview
on
how
to
configure
Spark

##
Contributing

Please
review
the
[Contribution
to
Spark
guide](http://spark
apache
org/contributing
html)
for
information
on
how
to
get
started
contributing
to
the
project

scala> val wordRDD = wordCandidateRDD.filter(_.matches("""\p{Alnum}+"""))
wordRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[25] at filter at <console>:25

scala> val wordAndOnePairRDD = wordRDD.map((_, 1))
wordAndOnePairRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[26] at map at <console>:25

scala> val wordAndCountRDD = wordAndOnePairRDD.reduceByKey(_ + _)
wordAndCountRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[27] at reduceByKey at <console>:25

scala> val wordAndCountArray = wordAndCountRDD.collect
wordAndCountArray: Array[(String, Int)] = Array((package,3), (this,1), (integration,1), (Python,4), (Because,1), (its,1), (guide,1), (There,1), (general,3), (have,1), (locally,3), (changed,1), (Java,1), (only,1), (several,1), (This,2), (basic,1), (Configuration,1), (documentation,4), (first,1), (graph,1), (Hive,2), (info,1), (prefer,1), (SparkPi,2), (Data,1), (engine,1), (version,1), (file,1), (analysis,1), (test,1), (MASTER,1), (tips,1), (example,3), (are,1), (params,1), (provides,1), (refer,2), (configure,1), (Interactive,2), (can,7), (build,4), (when,1), (easiest,1), (Apache,1), (thread,2), (how,3), (Note,1), (IDE,1), (Scala,3), (variable,1), (them,1), (submit,1), (Testing,1), (Streaming,1), (Developer,1), (Version,1), (rich,1), (systems,1), (detailed,2), (stream,1), (GraphX,1), (dis...
scala> wordAndCountArray.foreach(println)
(package,3)
(this,1)
(integration,1)
(Python,4)
(Because,1)
(its,1)
(guide,1)
(There,1)
(general,3)
(have,1)
(locally,3)
(changed,1)
(Java,1)
(only,1)
(several,1)
(This,2)
(basic,1)
(Configuration,1)
(documentation,4)
(first,1)
(graph,1)
(Hive,2)
(info,1)
(prefer,1)
(SparkPi,2)
(Data,1)
(engine,1)
(version,1)
(file,1)
(analysis,1)
(test,1)
(MASTER,1)
(tips,1)
(example,3)
(are,1)
(params,1)
(provides,1)
(refer,2)
(configure,1)
(Interactive,2)
(can,7)
(build,4)
(when,1)
(easiest,1)
(Apache,1)
(thread,2)
(how,3)
(Note,1)
(IDE,1)
(Scala,3)
(variable,1)
(them,1)
(submit,1)
(Testing,1)
(Streaming,1)
(Developer,1)
(Version,1)
(rich,1)
(systems,1)
(detailed,2)
(stream,1)
(GraphX,1)
(distribution,1)
(review,1)
(Please,4)
(return,2)
(is,7)
(Thriftserver,1)
(Alternatively,1)
(R,1)
(runs,1)
(start,1)
(same,1)
(built,2)
(one,3)
(with,4)
(data,1)
(Kubernetes,1)
(Contributing,1)
(using,5)
(talk,1)
(Shell,2)
(class,2)
(sc,2)
(Enabling,1)
(README,1)
(computing,1)
(module,1)
(from,1)
(building,2)
(set,2)
(other,1)
(N,1)
(Example,1)
(learning,1)
(Building,1)
(need,1)
(Big,1)
(fast,1)
(guidance,2)
(uses,1)
(SQL,2)
(will,1)
(information,1)
(YARN,1)
(requires,1)
(get,1)
(apache,10)
(Documentation,1)
(web,1)
(cluster,3)
(MLlib,1)
(DataFrames,1)
(contributing,1)
(supports,2)
(sample,1)
(For,3)
(Programs,1)
(Spark,17)
(particular,2)
(The,1)
(than,1)
(APIs,1)
(computation,1)
(Try,1)
(library,1)
(A,1)
(through,1)
(following,2)
(More,1)
(which,2)
(md,1)
(threads,1)
(also,5)
(storage,1)
(should,2)
(To,2)
(for,12)
(Once,1)
(setup,1)
(latest,1)
(your,1)
(the,24)
(URL,1)
(not,1)
(different,1)
(About,1)
(if,4)
(given,1)
(site,1)
(be,2)
(do,2)
(Tests,1)
(no,1)
(including,4)
(distributions,1)
(Versions,1)
(started,1)
(HDFS,1)
(by,1)
(individual,1)
(It,2)
(Maven,2)
(an,4)
(programming,1)
(machine,1)
(environment,1)
(clean,1)
(And,1)
(developing,1)
(run,7)
(on,7)
(You,4)
(instructions,1)
(against,1)
(help,1)
(print,1)
(tests,2)
(examples,2)
(at,2)
(in,6)
(optimized,1)
(development,1)
(downloaded,1)
(graphs,1)
(versions,1)
(usage,1)
(builds,1)
(online,1)
(abbreviated,1)
(comes,1)
(overview,1)
(Many,1)
(Running,1)
(way,1)
(use,3)
(Online,1)
(running,1)
(find,1)
(command,2)
(contains,1)
(directory,1)
(project,2)
(you,4)
(Pi,1)
(that,2)
(protocols,1)
(a,9)
(or,3)
(name,1)
(processing,2)
(to,17)
(available,1)
(core,1)
(more,1)
(see,4)
(of,5)
(tools,1)
(programs,3)
(option,1)
(must,1)
(and,10)
(system,1)
(Hadoop,5)

scala> val textRDD = sc.textFile("\path/to/edudata/product_click.log")
<console>:1: error: invalid escape character
val textRDD = sc.textFile("\path/to/edudata/product_click.log")
                            ^

scala> val textRDD = sc.textFile("/path/to/edudata/product_click.log")
textRDD: org.apache.spark.rdd.RDD[String] = /path/to/edudata/product_click.log MapPartitionsRDD[29] at textFile at <console>:24

scala> val textArray = textRDD.collect
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://master:9000/path/to/edudata/product_click.log
  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
  ... 49 elided

scala> val textRDD = sc.textFile("/path/edudata/product_click.log")
textRDD: org.apache.spark.rdd.RDD[String] = /path/edudata/product_click.log MapPartitionsRDD[31] at textFile at <console>:24

scala> val textArray = textRDD.collect
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://master:9000/path/edudata/product_click.log
  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
  ... 49 elided

scala> val textRDD = sc.textFile("file:///hdfs dfs/edudata/product_click.log")
textRDD: org.apache.spark.rdd.RDD[String] = file:///hdfs dfs/edudata/product_click.log MapPartitionsRDD[33] at textFile at <console>:24

scala> val textArray = textRDD.collect
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/hdfs dfs/edudata/product_click.log
  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
  ... 49 elided

scala> val textRDD = sc.textFile("file:///edudata/product_click.log")
textRDD: org.apache.spark.rdd.RDD[String] = file:///edudata/product_click.log MapPartitionsRDD[35] at textFile at <console>:24

scala> val textArray = textRDD.collect
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/edudata/product_click.log
  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
  ... 49 elided

scala> val textRDD = sc.textFile("file:///hdfs/edudata/product_click.log")
textRDD: org.apache.spark.rdd.RDD[String] = file:///hdfs/edudata/product_click.log MapPartitionsRDD[37] at textFile at <console>:24

scala> val textArray = textRDD.collect
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/hdfs/edudata/product_click.log
  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
  ... 49 elided

scala> val textRDD = sc.textFile("/path/to/edudata/product_click.log")
textRDD: org.apache.spark.rdd.RDD[String] = /path/to/edudata/product_click.log MapPartitionsRDD[39] at textFile at <console>:24

scala> val textArray = textRDD.collect
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://master:9000/path/to/edudata/product_click.log
  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
  ... 49 elided

scala> val textRDD = sc.textFile("edudata/product_click.log")
textRDD: org.apache.spark.rdd.RDD[String] = edudata/product_click.log MapPartitionsRDD[41] at textFile at <console>:24

scala> val textArray = textRDD.collect
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://master:9000/user/root/edudata/product_click.log
  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)
  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)
  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
  ... 49 elided

scala> val textRDD = sc.textFile("/edudata/product_click.log")
textRDD: org.apache.spark.rdd.RDD[String] = /edudata/product_click.log MapPartitionsRDD[43] at textFile at <console>:24

scala> val textArray = textRDD.collect
textArray: Array[String] = Array(201612120944 p001, 201612120944 p003, 201612120944 p011, 201612120945 p008, 201612121052 p008, 201612121052 p011, 201612121052 p014, 201612121115 p015, 201612121116 p009, 201612121344 p001, 201612121344 p003, 201612121344 p011, 201612121345 p008, 201612121452 p008, 201612121452 p011, 201612121452 p014, 201612121615 p015, 201612121616 p009, 201612121622 p005, 201612121622 p001, 201612121622 p007, 201612121640 p004, 201612121640 p011, 201612130925 p002, 201612130925 p011, 201612130925 p012, 201612131024 p010, 201612131024 p014, 201612131332 p003, 201612131332 p008, 201612131332 p013, 201612131332 p012, 201612131332 p014, 201612131536 p004, 201612131536 p005, 201612131536 p007, 201612131536 p009, 201612131537 p008, 201612131742 p006, 201612131742 p009, 2016...
scala> textArray.foreach(println)
201612120944 p001

201708121052 p011
201708121052 p014
201708121115 p015
201708121116 p009

scala> val isWord: String => Boolean = word => word.matches("""\p{Alnum}+""")
isWord: String => Boolean = <function1>

scala> val wordRDD = textRDD.filter(isWord)
wordRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[44] at filter at <console>:27

scala> val wordArray = wordRDD.collect
wordArray: Array[String] = Array()

scala> wordArray.foreach(println)

scala> wordArray.foreach(println)

scala> val textRDD = sc.textFile("/edudata/product_click.log")
textRDD: org.apache.spark.rdd.RDD[String] = /edudata/product_click.log MapPartitionsRDD[46] at textFile at <console>:24

scala> val textArray = textRDD.collect
textArray: Array[String] = Array(201612120944 p001, 201612120944 p003, 201612120944 p011, 201612120945 p008, 201612121052 p008, 201612121052 p011, 201612121052 p014, 201612121115 p015, 201612121116 p009, 201612121344 p001, 201612121344 p003, 201612121344 p011, 201612121345 p008, 201612121452 p008, 201612121452 p011, 201612121452 p014, 201612121615 p015, 201612121616 p009, 201612121622 p005, 201612121622 p001, 201612121622 p007, 201612121640 p004, 201612121640 p011, 201612130925 p002, 201612130925 p011, 201612130925 p012, 201612131024 p010, 201612131024 p014, 201612131332 p003, 201612131332 p008, 201612131332 p013, 201612131332 p012, 201612131332 p014, 201612131536 p004, 201612131536 p005, 201612131536 p007, 201612131536 p009, 201612131537 p008, 201612131742 p006, 201612131742 p009, 2016...
scala> textArray.foreach(println)
201612120944 p001

201708121052 p011
201708121052 p014
201708121115 p015
201708121116 p009

scala> val log = textArray.filter(_.matches("""\p{Alnum}+"""))
log: Array[String] = Array()

scala> first(textArray)
<console>:26: error: overloaded method value first with alternatives:
  (columnName: String)org.apache.spark.sql.Column <and>
  (e: org.apache.spark.sql.Column)org.apache.spark.sql.Column
 cannot be applied to (Array[String])
       first(textArray)
       ^

scala> val logRDD = log.map((_,1))
logRDD: Array[(String, Int)] = Array()

scala> val countRDD = logRDD.reduceByKey(_ + _)
<console>:25: error: value reduceByKey is not a member of Array[(String, Int)]
       val countRDD = logRDD.reduceByKey(_ + _)
                             ^

scala> val textRDD = sc.textFile("/edudata/product_click.log")
textRDD: org.apache.spark.rdd.RDD[String] = /edudata/product_click.log MapPartitionsRDD[48] at textFile at <console>:24

scala> val textArray = textRDD.collect
textArray: Array[String] = Array(201612120944 p001, 201612120944 p003, 201612120944 p011, 201612120945 p008, 201612121052 p008, 201612121052 p011, 201612121052 p014, 201612121115 p015, 201612121116 p009, 201612121344 p001, 201612121344 p003, 201612121344 p011, 201612121345 p008, 201612121452 p008, 201612121452 p011, 201612121452 p014, 201612121615 p015, 201612121616 p009, 201612121622 p005, 201612121622 p001, 201612121622 p007, 201612121640 p004, 201612121640 p011, 201612130925 p002, 201612130925 p011, 201612130925 p012, 201612131024 p010, 201612131024 p014, 201612131332 p003, 201612131332 p008, 201612131332 p013, 201612131332 p012, 201612131332 p014, 201612131536 p004, 201612131536 p005, 201612131536 p007, 201612131536 p009, 201612131537 p008, 201612131742 p006, 201612131742 p009, 2016...
scala> textRDD.first
res26: String = 201612120944 p001

scala> val wordRDD = textRDD.filter(word => word.matches("""\p{Alnum}+"""))
wordRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[49] at filter at <console>:25

scala> val wordArray = wordRDD.collect
wordArray: Array[String] = Array()

scala> wordArray.foreach(println)

scala> 

scala> val textRDD = sc.textFile("/edudata/product_click.log")
textRDD: org.apache.spark.rdd.RDD[String] = /edudata/product_click.log MapPartitionsRDD[51] at textFile at <console>:24

scala> val textArray = textRDD.collect
textArray: Array[String] = Array(201612120944 p001, 201612120944 p003, 201612120944 p011, 201612120945 p008, 201612121052 p008, 201612121052 p011, 201612121052 p014, 201612121115 p015, 201612121116 p009, 201612121344 p001, 201612121344 p003, 201612121344 p011, 201612121345 p008, 201612121452 p008, 201612121452 p011, 201612121452 p014, 201612121615 p015, 201612121616 p009, 201612121622 p005, 201612121622 p001, 201612121622 p007, 201612121640 p004, 201612121640 p011, 201612130925 p002, 201612130925 p011, 201612130925 p012, 201612131024 p010, 201612131024 p014, 201612131332 p003, 201612131332 p008, 201612131332 p013, 201612131332 p012, 201612131332 p014, 201612131536 p004, 201612131536 p005, 201612131536 p007, 201612131536 p009, 201612131537 p008, 201612131742 p006, 201612131742 p009, 2016...
scala> val wordRDD = textRDD.filter(word => word.matches("""\p{Alnum}*"""))
wordRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[52] at filter at <console>:25

scala> val wordArray = wordRDD.collect
wordArray: Array[String] = Array()

scala> wordArray.foreach(println)

scala> val isWord: String,Int => Boolean = word => word.matches("""\p{Alnum}+""")
<console>:1: error: ';' expected but ',' found.
val isWord: String,Int => Boolean = word => word.matches("""\p{Alnum}+""")
                  ^

scala> textArray.first
<console>:26: error: value first is not a member of Array[String]
       textArray.first
                 ^

scala> val textRDD = sc.textFile("/edudata/product_click.log")
textRDD: org.apache.spark.rdd.RDD[String] = /edudata/product_click.log MapPartitionsRDD[54] at textFile at <console>:24

scala> val textArray = textRDD.collect
textArray: Array[String] = Array(201612120944 p001, 201612120944 p003, 201612120944 p011, 201612120945 p008, 201612121052 p008, 201612121052 p011, 201612121052 p014, 201612121115 p015, 201612121116 p009, 201612121344 p001, 201612121344 p003, 201612121344 p011, 201612121345 p008, 201612121452 p008, 201612121452 p011, 201612121452 p014, 201612121615 p015, 201612121616 p009, 201612121622 p005, 201612121622 p001, 201612121622 p007, 201612121640 p004, 201612121640 p011, 201612130925 p002, 201612130925 p011, 201612130925 p012, 201612131024 p010, 201612131024 p014, 201612131332 p003, 201612131332 p008, 201612131332 p013, 201612131332 p012, 201612131332 p014, 201612131536 p004, 201612131536 p005, 201612131536 p007, 201612131536 p009, 201612131537 p008, 201612131742 p006, 201612131742 p009, 2016...
scala> textArray.first
<console>:26: error: value first is not a member of Array[String]
       textArray.first
                 ^

scala> textArray.foreach(println)
201612120944 p001

201708121052 p014
201708121115 p015
201708121116 p009

scala> textArray.first
<console>:26: error: value first is not a member of Array[String]
       textArray.first
                 ^

scala> textRDD.first
res33: String = 201612120944 p001

scala> 

```

